{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167f0e06-e9ff-4bd3-a799-e2d34b344c9b",
   "metadata": {},
   "source": [
    "# Example code for DigiDiaDem Dataset\n",
    "\n",
    "This notebook contains example code for dataset and experiments as described in [The DigiDiaDem Speech-Cognitive Dataset: Initial Experiments on Detecting Cognitive Impairments from Speech](https://example.com).\n",
    "\n",
    "Dataset used in this notebook is publicly available [here](http://hdl.handle.net/11234/1-6043)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e9665-1c0c-463e-9de5-4c14feca56c6",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ff8c27-c4af-4ab4-97ab-e7330a2af7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required packages and objects\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927221e6-fa63-4962-bcd3-fc9c172ecb50",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e9829b-2cc5-4015-9d92-2a93205e8df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1059k    0 1059k    0     0  2693k      0 --:--:-- --:--:-- --:--:-- 2695k\n",
      "Archive:  allzip.zip\n",
      "  inflating: DigiDiademSpeechCognitiveDataset.zip  \n",
      "  inflating: DigiDiaDemSpeechCognitiveDataset.md  \n",
      "Archive:  DigiDiademSpeechCognitiveDataset.zip\n",
      "  inflating: ddd.yaml                \n",
      "  inflating: expert_features_zipformer_lm-extra06.json  \n",
      "  inflating: expert_scores.json      \n",
      "  inflating: metadata_20251031.json  \n",
      "  inflating: recordings_20251031.json  \n",
      "  inflating: sessions_20251031.json  \n",
      "  inflating: test_20251031.json      \n",
      "  inflating: train_20251031.json     \n",
      "  inflating: transcriptions_annotation_20251031.json  \n",
      "  inflating: transcriptions_zipformer_20251031.json  \n",
      "  inflating: transcriptions_zipformer_lm-extra06_20251031.json  \n"
     ]
    }
   ],
   "source": [
    "# download and unzip the dataset\n",
    "!curl -o allzip.zip https://lindat.mff.cuni.cz/repository/server/api/core/items/f4db9410-a3e9-4d8f-81ab-7c7dea4206bf/allzip?handleId=11234/1-6043\n",
    "!unzip -o allzip.zip\n",
    "!unzip -o DigiDiademSpeechCognitiveDataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5502799-e1e3-4f0f-bed8-f5af55e1a048",
   "metadata": {},
   "source": [
    "### Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e8e777-cdde-489c-9bc7-6d839483b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-load all of the required data for dataset construction\n",
    "with open(\"metadata_20251031.json\", \"r\") as f:\n",
    "    meta = pd.DataFrame(json.load(f)).set_index(\"screening_id\")\n",
    "with open(\"expert_features_zipformer_lm-extra06.json\", \"r\") as f:\n",
    "    features_raw = pd.DataFrame(json.load(f)).set_index(\"screening_id\")\n",
    "with open(\"train_20251031.json\", \"r\") as f:\n",
    "    train_ids = json.load(f)\n",
    "with open(\"test_20251031.json\", \"r\") as f:\n",
    "    test_ids = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158fdb9-4a27-4eb4-b8ea-427159c52206",
   "metadata": {},
   "source": [
    "### Define helper functions\n",
    "1. `get_xy(feature_names, target)`: Filter the dataset by list of feature names and create Xy split for given target string.\n",
    "2. `train_and_eval_logreg(x_train, y_train, x_test, y_test, crossval)`: Train Logistic Regression classifier and evaluate it's performance on test data, optionally run crossvalidation (using both subsets)\n",
    "3. `train_and_eval_histgradboost(x_train, y_train, x_test, y_test, crossval)`: Train Histogram Gradient Boosting classifier and evaluate it's performance on test data, optionally run crossvalidation (using both subsets)\n",
    "4. `sep()`: simple function that just prints a separator line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec35712-a4a4-40b5-b9cb-fab609b3979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(feature_names, target):\n",
    "    global features_raw\n",
    "    target_maps = {\"0vs23\": {0: 0, 2: 1, 3: 1}, \"0vs1vs2vs3\": {0: 0, 1: 1, 2: 2, 3: 3}}\n",
    "    targets = meta.loc[meta[\"kobar_kategorizace_definitivni\"].isin(target_maps[target]), \"kobar_kategorizace_definitivni\"].map(target_maps[target]).sort_index()\n",
    "    features = features_raw.loc[features_raw.index.intersection(targets.index)].sort_index()\n",
    "    x_train = features.loc[features.index.isin(train_ids)][feature_names]\n",
    "    x_test = features.loc[features.index.isin(test_ids)][feature_names]\n",
    "    y_train = targets.loc[targets.index.isin(train_ids)]\n",
    "    y_test = targets.loc[targets.index.isin(test_ids)]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def train_and_eval_logreg(x_train, y_train, x_test, y_test, crossval=False):\n",
    "    model = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"logreg\",OneVsRestClassifier(LogisticRegression(random_state=42, max_iter=1000, solver=\"liblinear\"))),\n",
    "    ])\n",
    "    if crossval:\n",
    "        x_full = pd.concat(x_train, x_test)\n",
    "        y_full = pd.concat(y_train, y_test)\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        predicted = cross_val_predict(model, x_full, y_full, cv=cv)\n",
    "        return clf_rep(y_true=y_full, y_pred=predicted, zero_division=0.0)\n",
    "    else:\n",
    "        model.fit(x_train, y_train)\n",
    "        predicted = model.predict(x_test)\n",
    "        return classification_report(y_test, predicted, zero_division=0.0)\n",
    "\n",
    "def train_and_eval_histgradboost(x_train, y_train, x_test, y_test, crossval=False):\n",
    "    model = HistGradientBoostingClassifier(random_state=42, early_stopping=False)\n",
    "    if crossval:\n",
    "        x_full = pd.concat(x_train, x_test)\n",
    "        y_full = pd.concat(y_train, y_test)\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        predicted = cross_val_predict(model, x_full, y_full, cv=cv)\n",
    "        return clf_rep(y_true=y_full, y_pred=predicted, zero_division=0.0)\n",
    "    else:\n",
    "        model.fit(x_train, y_train)\n",
    "        predicted = model.predict(x_test)\n",
    "        return classification_report(y_test, predicted, zero_division=0.0)\n",
    "\n",
    "def sep():\n",
    "    print('='*80+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1451d-db10-45dd-b6cb-28b3f2321a8d",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Following cells contain code for the experiments, separated by task.  \n",
    "\n",
    "Each cell defines list of feature names that should be included in the experiment, then x-y split id obtained and used to train and evaluate both **Logistic Regression** and **Histogram Gradient Boosting** classifiers.  \n",
    "Sample of the training data is printed to alow for manual inspection of the data and then classification reports are shown below. \n",
    "\n",
    "#### Target classes\n",
    "The original dataset contains 4 individual classes:\n",
    "- **class `0` (CN):** Community individuals with normal cognition. Common people, all of whose RBANS results were in the norm according to international guidelines.\n",
    "- **class `1` (CMCI):** Community individuals with possible mild cognitive impairment. Common people, all of whose RBANS results were declined.\n",
    "- **class `2` (MCI):** Patients with mild cognitive impairment with symptoms and objective cognitive impairment based on the judgement of a~cognitive neurologist with longtime experience.Most of these patients have been followed at the Memory Clinic after detailed diagnostic work-up, including clinical and neuroimaging evaluation. All the cognitive tests and questionnaires in the current study were not used for group categorization since they were collected after an invitation and an agreement to participate in our study by the cognitive neurologist.\n",
    "- **class `3`: (DEM):** Patients with mild dementia. Patients are dependent on caregiver help. The same principle of examination sequence and categorization as for MCI patients applies to these patients with mild dementia.\n",
    "\n",
    "A large number of experiments can be performed over a set of four target classes. For defining the baseline experiments, we present following groupings into different classification types:\n",
    "1. binary classification `0vs2+3` to separate community individuals with normal cognition from patients with mild cognitive impariments or mild dementia\n",
    "2. 4-class classification `0vs1vs2vs3` to explore the separability of each individual group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f65f3f-6a2d-4f60-aa21-c8379eb6d5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 01] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_1_task1 Correctly repeated numbers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_1_task1 Correctly repeated numbers\n",
       "screening_id                                                                 \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                9.0\n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                9.0\n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                9.0\n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                9.0\n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                9.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 01] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82        42\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.35      0.50      0.41        60\n",
      "weighted avg       0.49      0.70      0.58        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 01] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.98      0.82        42\n",
      "           1       0.50      0.06      0.10        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.60      0.52      0.46        60\n",
      "weighted avg       0.64      0.70      0.60        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 01] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.55        77\n",
      "   macro avg       0.14      0.25      0.18        77\n",
      "weighted avg       0.30      0.55      0.39        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 01] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.55        77\n",
      "   macro avg       0.14      0.25      0.18        77\n",
      "weighted avg       0.30      0.55      0.39        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 01                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = ['expertFeatures_1_task1 Correctly repeated numbers']\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 01] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 01] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 01] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 01] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 01] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d2cf8c-986d-4164-8425-c48ecabf3466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 02] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_2_task2 Correctly repeated characters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_2_task2 Correctly repeated characters\n",
       "screening_id                                                                    \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                4.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                4.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                4.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                4.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                3.0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 02] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82        42\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.35      0.50      0.41        60\n",
      "weighted avg       0.49      0.70      0.58        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 02] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82        42\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.35      0.50      0.41        60\n",
      "weighted avg       0.49      0.70      0.58        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 02] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.55        77\n",
      "   macro avg       0.14      0.25      0.18        77\n",
      "weighted avg       0.30      0.55      0.39        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 02] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.55        77\n",
      "   macro avg       0.14      0.25      0.18        77\n",
      "weighted avg       0.30      0.55      0.39        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 02                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = ['expertFeatures_2_task2 Correctly repeated characters']\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 02] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 02] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 02] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 02] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 02] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cface6e4-883d-42cb-a3c7-ef2676875dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 03] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_3_task3 Character match ratio</th>\n",
       "      <th>expertFeatures_4_task3 Correctly repeated words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>0.4490</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>0.9434</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>0.4227</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>0.6792</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_3_task3 Character match ratio  \\\n",
       "screening_id                                                               \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                        0.4490   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                        0.9434   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                        0.4227   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                        0.6792   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                        1.0000   \n",
       "\n",
       "                            expertFeatures_4_task3 Correctly repeated words  \n",
       "screening_id                                                                 \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                              6.0  \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                              9.0  \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                             10.0  \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                              9.0  \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                             10.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 03] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83        42\n",
      "           1       0.62      0.44      0.52        18\n",
      "\n",
      "    accuracy                           0.75        60\n",
      "   macro avg       0.70      0.66      0.67        60\n",
      "weighted avg       0.74      0.75      0.74        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 03] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        42\n",
      "           1       0.75      0.67      0.71        18\n",
      "\n",
      "    accuracy                           0.83        60\n",
      "   macro avg       0.81      0.79      0.79        60\n",
      "weighted avg       0.83      0.83      0.83        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 03] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.95      0.72        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.38      0.33      0.35         9\n",
      "\n",
      "    accuracy                           0.56        77\n",
      "   macro avg       0.24      0.32      0.27        77\n",
      "weighted avg       0.36      0.56      0.43        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 03] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.86      0.73        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.33      0.22      0.27         9\n",
      "           3       0.36      0.44      0.40         9\n",
      "\n",
      "    accuracy                           0.55        77\n",
      "   macro avg       0.33      0.38      0.35        77\n",
      "weighted avg       0.43      0.55      0.48        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 03                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_3_task3 Character match ratio',\n",
    "\t'expertFeatures_4_task3 Correctly repeated words'\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 03] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 03] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 03] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 03] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 03] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cfac83b-83ca-4474-ae3f-d01b757d03e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 04m] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_5_task4 Sentence count</th>\n",
       "      <th>expertFeatures_6_task4 First person verb proportion</th>\n",
       "      <th>expertFeatures_7_task4 Meaningful words ratio</th>\n",
       "      <th>expertFeatures_8_task4 Pronoun to noun ratio</th>\n",
       "      <th>expertFeatures_9_task4 Count of repeated meaningful words</th>\n",
       "      <th>expertFeatures_10_task4 Unique words to total words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.638554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.587302</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_5_task4 Sentence count  \\\n",
       "screening_id                                                        \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                    6.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                    3.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                   10.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                   15.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                   15.0   \n",
       "\n",
       "                            expertFeatures_6_task4 First person verb proportion  \\\n",
       "screening_id                                                                      \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                           0.000000     \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                           0.166667     \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                           0.000000     \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                           0.043478     \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                           0.000000     \n",
       "\n",
       "                            expertFeatures_7_task4 Meaningful words ratio  \\\n",
       "screening_id                                                                \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                       0.573770   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                       0.647059   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                       0.595960   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                       0.587302   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                       0.538462   \n",
       "\n",
       "                            expertFeatures_8_task4 Pronoun to noun ratio  \\\n",
       "screening_id                                                               \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                      0.173913   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                      0.071429   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                      0.028571   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                      0.242424   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                      0.217391   \n",
       "\n",
       "                            expertFeatures_9_task4 Count of repeated meaningful words  \\\n",
       "screening_id                                                                            \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                6.0           \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                3.0           \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                               13.0           \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                               17.0           \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                8.0           \n",
       "\n",
       "                            expertFeatures_10_task4 Unique words to total words  \n",
       "screening_id                                                                     \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                           0.769231    \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                           0.896552    \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                           0.638554    \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                           0.615385    \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                           0.836066    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 04m] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82        42\n",
      "           1       0.57      0.44      0.50        18\n",
      "\n",
      "    accuracy                           0.73        60\n",
      "   macro avg       0.68      0.65      0.66        60\n",
      "weighted avg       0.72      0.73      0.72        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 04m] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        42\n",
      "           1       0.50      0.50      0.50        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.64      0.64      0.64        60\n",
      "weighted avg       0.70      0.70      0.70        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 04m] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.93      0.70        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.33      0.22      0.27         9\n",
      "\n",
      "    accuracy                           0.53        77\n",
      "   macro avg       0.22      0.29      0.24        77\n",
      "weighted avg       0.35      0.53      0.41        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 04m] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.76      0.67        42\n",
      "           1       0.27      0.18      0.21        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.60      0.33      0.43         9\n",
      "\n",
      "    accuracy                           0.49        77\n",
      "   macro avg       0.37      0.32      0.33        77\n",
      "weighted avg       0.45      0.49      0.46        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 04m                          #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_5_task4 Sentence count',\n",
    "\t'expertFeatures_6_task4 First person verb proportion',\n",
    "\t'expertFeatures_7_task4 Meaningful words ratio',\n",
    "\t'expertFeatures_8_task4 Pronoun to noun ratio',\n",
    "\t'expertFeatures_9_task4 Count of repeated meaningful words',\n",
    "\t'expertFeatures_10_task4 Unique words to total words',\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 04m] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 04m] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 04m] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 04m] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 04m] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f87b2e-0a8b-4b56-b5a8-5e99af0768e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 04s] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_11_task4 Named object count</th>\n",
       "      <th>expertFeatures_12_task4 Described object relation count</th>\n",
       "      <th>expertFeatures_13_task4 Distinct topic count</th>\n",
       "      <th>expertFeatures_14_task4 Description trajectory length</th>\n",
       "      <th>expertFeatures_15_task4 Objects in water count</th>\n",
       "      <th>expertFeatures_16_task4 Objects in sky count</th>\n",
       "      <th>expertFeatures_17_task4 Objects on land count</th>\n",
       "      <th>expertFeatures_18_task4 Explicit child danger mentioned</th>\n",
       "      <th>expertFeatures_19_task4 Explicit animal danger mentioned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.866285</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.594183</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.319715</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.277062</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.562818</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_11_task4 Named object count  \\\n",
       "screening_id                                                             \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                        21.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                        13.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                        33.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                        24.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                        21.0   \n",
       "\n",
       "                            expertFeatures_12_task4 Described object relation count  \\\n",
       "screening_id                                                                          \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                               13.0         \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                8.0         \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                5.0         \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                5.0         \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                4.0         \n",
       "\n",
       "                            expertFeatures_13_task4 Distinct topic count  \\\n",
       "screening_id                                                               \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                          14.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                          13.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                          24.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                          16.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                          14.0   \n",
       "\n",
       "                            expertFeatures_14_task4 Description trajectory length  \\\n",
       "screening_id                                                                        \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                           0.866285       \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                           0.594183       \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                           2.319715       \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                           1.277062       \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                           1.562818       \n",
       "\n",
       "                            expertFeatures_15_task4 Objects in water count  \\\n",
       "screening_id                                                                 \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                             7.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                             3.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                             8.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                             5.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                             8.0   \n",
       "\n",
       "                            expertFeatures_16_task4 Objects in sky count  \\\n",
       "screening_id                                                               \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                           3.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                           2.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                           5.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                           5.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                           3.0   \n",
       "\n",
       "                            expertFeatures_17_task4 Objects on land count  \\\n",
       "screening_id                                                                \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                            5.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                            3.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                           11.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                            9.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                            4.0   \n",
       "\n",
       "                            expertFeatures_18_task4 Explicit child danger mentioned  \\\n",
       "screening_id                                                                          \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                0.0         \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                1.0         \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                0.0         \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                1.0         \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                1.0         \n",
       "\n",
       "                            expertFeatures_19_task4 Explicit animal danger mentioned  \n",
       "screening_id                                                                          \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                1.0         \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                0.0         \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                1.0         \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                1.0         \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                1.0         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 04s] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82        42\n",
      "           1       0.59      0.56      0.57        18\n",
      "\n",
      "    accuracy                           0.75        60\n",
      "   macro avg       0.70      0.69      0.70        60\n",
      "weighted avg       0.75      0.75      0.75        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 04s] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79        42\n",
      "           1       0.50      0.44      0.47        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.64      0.63      0.63        60\n",
      "weighted avg       0.69      0.70      0.69        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 04s] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.95      0.75        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.40      0.44      0.42         9\n",
      "\n",
      "    accuracy                           0.57        77\n",
      "   macro avg       0.26      0.35      0.29        77\n",
      "weighted avg       0.39      0.57      0.46        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 04s] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64        42\n",
      "           1       0.43      0.18      0.25        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.25      0.22      0.24         9\n",
      "\n",
      "    accuracy                           0.47        77\n",
      "   macro avg       0.31      0.28      0.28        77\n",
      "weighted avg       0.43      0.47      0.43        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 04s                          #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_11_task4 Named object count',\n",
    "\t'expertFeatures_12_task4 Described object relation count',\n",
    "\t'expertFeatures_13_task4 Distinct topic count',\n",
    "\t'expertFeatures_14_task4 Description trajectory length',\n",
    "\t'expertFeatures_15_task4 Objects in water count',\n",
    "\t'expertFeatures_16_task4 Objects in sky count',\n",
    "\t'expertFeatures_17_task4 Objects on land count',\n",
    "\t'expertFeatures_18_task4 Explicit child danger mentioned',\n",
    "\t'expertFeatures_19_task4 Explicit animal danger mentioned',\n",
    " ]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 04s] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 04s] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 04s] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 04s] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 04s] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9f27fb-2ddf-4335-b6ef-4dc7dff9ee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 05] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_20_task5 Total recalled words count</th>\n",
       "      <th>expertFeatures_21_task5 Distinct objects recalled count</th>\n",
       "      <th>expertFeatures_22_task5 Repeated recalled words count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>42.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>25.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_20_task5 Total recalled words count  \\\n",
       "screening_id                                                                     \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                               42.0    \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                               25.0    \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                               28.0    \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                               20.0    \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                               24.0    \n",
       "\n",
       "                            expertFeatures_21_task5 Distinct objects recalled count  \\\n",
       "screening_id                                                                          \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                               11.0         \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                               11.0         \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                9.0         \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                3.0         \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                               11.0         \n",
       "\n",
       "                            expertFeatures_22_task5 Repeated recalled words count  \n",
       "screening_id                                                                       \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                9.0      \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                4.0      \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                0.0      \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                2.0      \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                4.0      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 05] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90        42\n",
      "           1       0.85      0.61      0.71        18\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.85      0.78      0.80        60\n",
      "weighted avg       0.85      0.85      0.84        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 05] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91        42\n",
      "           1       0.92      0.61      0.73        18\n",
      "\n",
      "    accuracy                           0.87        60\n",
      "   macro avg       0.89      0.79      0.82        60\n",
      "weighted avg       0.87      0.87      0.86        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 05] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.58      0.78      0.67         9\n",
      "\n",
      "    accuracy                           0.64        77\n",
      "   macro avg       0.31      0.44      0.36        77\n",
      "weighted avg       0.42      0.64      0.51        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 05] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74        42\n",
      "           1       0.50      0.53      0.51        17\n",
      "           2       0.33      0.11      0.17         9\n",
      "           3       0.56      0.56      0.56         9\n",
      "\n",
      "    accuracy                           0.62        77\n",
      "   macro avg       0.52      0.50      0.49        77\n",
      "weighted avg       0.60      0.62      0.60        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 05                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_20_task5 Total recalled words count',\n",
    "\t'expertFeatures_21_task5 Distinct objects recalled count',\n",
    "\t'expertFeatures_22_task5 Repeated recalled words count',\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 05] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 05] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 05] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 05] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 05] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48d9aacf-f402-43a9-be1a-10367467f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 06] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_23_task6 Correctly named pictures count</th>\n",
       "      <th>expertFeatures_24_task6 Total naming reaction time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>15.0</td>\n",
       "      <td>73.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>17.0</td>\n",
       "      <td>57.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>16.0</td>\n",
       "      <td>59.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>17.0</td>\n",
       "      <td>57.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>19.0</td>\n",
       "      <td>50.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_23_task6 Correctly named pictures count  \\\n",
       "screening_id                                                                         \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                               15.0        \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                               17.0        \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                               16.0        \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                               17.0        \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                               19.0        \n",
       "\n",
       "                            expertFeatures_24_task6 Total naming reaction time  \n",
       "screening_id                                                                    \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                              73.88   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                              57.04   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                              59.60   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                              57.68   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                              50.80   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 06] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84        42\n",
      "           1       0.67      0.44      0.53        18\n",
      "\n",
      "    accuracy                           0.77        60\n",
      "   macro avg       0.73      0.67      0.69        60\n",
      "weighted avg       0.75      0.77      0.75        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 06] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85        42\n",
      "           1       0.69      0.50      0.58        18\n",
      "\n",
      "    accuracy                           0.78        60\n",
      "   macro avg       0.75      0.70      0.72        60\n",
      "weighted avg       0.77      0.78      0.77        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 06] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.95      0.73        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.30      0.33      0.32         9\n",
      "\n",
      "    accuracy                           0.56        77\n",
      "   macro avg       0.22      0.32      0.26        77\n",
      "weighted avg       0.36      0.56      0.44        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 06] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71        42\n",
      "           1       0.25      0.06      0.10        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.29      0.44      0.35         9\n",
      "\n",
      "    accuracy                           0.52        77\n",
      "   macro avg       0.29      0.33      0.29        77\n",
      "weighted avg       0.43      0.52      0.45        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 06                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_23_task6 Correctly named pictures count',\n",
    "\t'expertFeatures_24_task6 Total naming reaction time',\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 06] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 06] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 06] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 06] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 06] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d757c35a-f78c-4e92-9b43-89ae2600448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 07] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_25_task7 Correctly recalled pictures count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_25_task7 Correctly recalled pictures count\n",
       "screening_id                                                                         \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                8.0        \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                4.0        \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                8.0        \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                1.0        \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                5.0        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 07] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89        42\n",
      "           1       0.76      0.72      0.74        18\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.82      0.81      0.82        60\n",
      "weighted avg       0.85      0.85      0.85        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 07] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86        42\n",
      "           1       0.65      0.83      0.73        18\n",
      "\n",
      "    accuracy                           0.82        60\n",
      "   macro avg       0.79      0.82      0.80        60\n",
      "weighted avg       0.84      0.82      0.82        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 07] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.95      0.76        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.64      1.00      0.78         9\n",
      "\n",
      "    accuracy                           0.64        77\n",
      "   macro avg       0.32      0.49      0.39        77\n",
      "weighted avg       0.42      0.64      0.51        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 07] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.86      0.73        42\n",
      "           1       0.14      0.06      0.08        17\n",
      "           2       0.40      0.22      0.29         9\n",
      "           3       0.89      0.89      0.89         9\n",
      "\n",
      "    accuracy                           0.61        77\n",
      "   macro avg       0.52      0.51      0.50        77\n",
      "weighted avg       0.53      0.61      0.56        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 07                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = ['expertFeatures_25_task7 Correctly recalled pictures count']\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 07] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 07] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 07] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 07] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 07] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8561d4e0-bb7d-498b-88a9-d33090d4bdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 08] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_26_task8 Total word count</th>\n",
       "      <th>expertFeatures_27_task8 Animal word count</th>\n",
       "      <th>expertFeatures_28_task8 Repeated animals count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_26_task8 Total word count  \\\n",
       "screening_id                                                           \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                      20.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                      16.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                      19.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                       8.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                      11.0   \n",
       "\n",
       "                            expertFeatures_27_task8 Animal word count  \\\n",
       "screening_id                                                            \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                       18.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                       10.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                       12.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                        6.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                       12.0   \n",
       "\n",
       "                            expertFeatures_28_task8 Repeated animals count  \n",
       "screening_id                                                                \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                             0.0  \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                             0.0  \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                             0.0  \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                             0.0  \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                             0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 08] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92        42\n",
      "           1       0.79      0.83      0.81        18\n",
      "\n",
      "    accuracy                           0.88        60\n",
      "   macro avg       0.86      0.87      0.86        60\n",
      "weighted avg       0.89      0.88      0.88        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 08] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92        42\n",
      "           1       0.82      0.78      0.80        18\n",
      "\n",
      "    accuracy                           0.88        60\n",
      "   macro avg       0.87      0.85      0.86        60\n",
      "weighted avg       0.88      0.88      0.88        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 08] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.98      0.77        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.62      0.89      0.73         9\n",
      "\n",
      "    accuracy                           0.64        77\n",
      "   macro avg       0.31      0.47      0.38        77\n",
      "weighted avg       0.42      0.64      0.51        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 08] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.88      0.76        42\n",
      "           1       0.20      0.06      0.09        17\n",
      "           2       0.40      0.44      0.42         9\n",
      "           3       0.71      0.56      0.62         9\n",
      "\n",
      "    accuracy                           0.61        77\n",
      "   macro avg       0.50      0.48      0.47        77\n",
      "weighted avg       0.54      0.61      0.56        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 08                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_26_task8 Total word count',\n",
    "\t'expertFeatures_27_task8 Animal word count',\n",
    "\t'expertFeatures_28_task8 Repeated animals count',\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 08] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 08] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 08] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 08] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 08] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80d9893e-a985-415e-9a21-a1e72d188c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 09] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_29_task9 Percentage of repeated-recalled sentence characters</th>\n",
       "      <th>expertFeatures_30_task9 Correct recalled sentence words count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>0.2830</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>0.4528</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>0.2073</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>0.1698</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>0.3623</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_29_task9 Percentage of repeated-recalled sentence characters  \\\n",
       "screening_id                                                                                              \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                             0.2830                             \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                             0.4528                             \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                             0.2073                             \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                             0.1698                             \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                             0.3623                             \n",
       "\n",
       "                            expertFeatures_30_task9 Correct recalled sentence words count  \n",
       "screening_id                                                                               \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                2.0              \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                4.0              \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                0.0              \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                0.0              \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                5.0              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 09] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89        42\n",
      "           1       0.76      0.72      0.74        18\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.82      0.81      0.82        60\n",
      "weighted avg       0.85      0.85      0.85        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 09] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92        42\n",
      "           1       0.87      0.72      0.79        18\n",
      "\n",
      "    accuracy                           0.88        60\n",
      "   macro avg       0.88      0.84      0.85        60\n",
      "weighted avg       0.88      0.88      0.88        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 09] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.98      0.80        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.56      1.00      0.72         9\n",
      "\n",
      "    accuracy                           0.65        77\n",
      "   macro avg       0.31      0.49      0.38        77\n",
      "weighted avg       0.43      0.65      0.52        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 09] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.72        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.20      0.11      0.14         9\n",
      "           3       0.67      0.89      0.76         9\n",
      "\n",
      "    accuracy                           0.57        77\n",
      "   macro avg       0.38      0.46      0.41        77\n",
      "weighted avg       0.45      0.57      0.50        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    ", crossval=True###############################################################\n",
    "#                           TASK 09                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_29_task9 Percentage of repeated-recalled sentence characters',\n",
    "\t'expertFeatures_30_task9 Correct recalled sentence words count',\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 09] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 09] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 09] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 09] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 09] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1006aacf-acc1-4d85-b608-409cc2a1bcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 10] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_31_task10 Word similarity score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>0.8040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>0.8768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>0.7285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>0.5230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_31_task10 Word similarity score\n",
       "screening_id                                                              \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                          0.8040\n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                          0.8768\n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                          0.7285\n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                             NaN\n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                          0.5230"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 10] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.93      0.80        42\n",
      "           1       0.25      0.06      0.09        18\n",
      "\n",
      "    accuracy                           0.67        60\n",
      "   macro avg       0.47      0.49      0.44        60\n",
      "weighted avg       0.56      0.67      0.58        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 10] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.98      0.81        42\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.68        60\n",
      "   macro avg       0.35      0.49      0.41        60\n",
      "weighted avg       0.49      0.68      0.57        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 10] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.55        77\n",
      "   macro avg       0.14      0.25      0.18        77\n",
      "weighted avg       0.30      0.55      0.39        77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 10] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.95      0.70        42\n",
      "           1       0.00      0.00      0.00        17\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.52        77\n",
      "   macro avg       0.14      0.24      0.18        77\n",
      "weighted avg       0.30      0.52      0.38        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 10                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = ['expertFeatures_31_task10 Word similarity score']\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 10] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 10] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 10] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now do the same thing, but for different targets:\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs1vs2vs3\")\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 10] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "print(\"[Task 10] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
