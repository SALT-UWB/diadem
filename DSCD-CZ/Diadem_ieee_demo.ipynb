{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167f0e06-e9ff-4bd3-a799-e2d34b344c9b",
   "metadata": {},
   "source": [
    "# Example code for DigiDiaDem Dataset\n",
    "\n",
    "This notebook contains example code for dataset and experiments as described in [The DigiDiaDem Speech-Cognitive Dataset: Initial Experiments on Detecting Cognitive Impairments from Speech](https://example.com).\n",
    "\n",
    "Dataset used in this notebook is publicly available [here](http://hdl.handle.net/11234/1-6043)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff8c27-c4af-4ab4-97ab-e7330a2af7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required packages and objects\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e9829b-2cc5-4015-9d92-2a93205e8df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1059k    0 1059k    0     0  9841k      0 --:--:-- --:--:-- --:--:-- 9899k\n",
      "Archive:  allzip.zip\n",
      "  inflating: DigiDiademSpeechCognitiveDataset.zip  \n",
      "  inflating: DigiDiaDemSpeechCognitiveDataset.md  \n",
      "Archive:  DigiDiademSpeechCognitiveDataset.zip\n",
      "  inflating: ddd.yaml                \n",
      "  inflating: expert_features_zipformer_lm-extra06.json  \n",
      "  inflating: expert_scores.json      \n",
      "  inflating: metadata_20251031.json  \n",
      "  inflating: recordings_20251031.json  \n",
      "  inflating: sessions_20251031.json  \n",
      "  inflating: test_20251031.json      \n",
      "  inflating: train_20251031.json     \n",
      "  inflating: transcriptions_annotation_20251031.json  \n",
      "  inflating: transcriptions_zipformer_20251031.json  \n",
      "  inflating: transcriptions_zipformer_lm-extra06_20251031.json  \n"
     ]
    }
   ],
   "source": [
    "# download and unzip the dataset\n",
    "!curl -o allzip.zip https://lindat.mff.cuni.cz/repository/server/api/core/items/f4db9410-a3e9-4d8f-81ab-7c7dea4206bf/allzip?handleId=11234/1-6043\n",
    "!unzip -o allzip.zip\n",
    "!unzip -o DigiDiademSpeechCognitiveDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "29e8e777-cdde-489c-9bc7-6d839483b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-load all of the required data for dataset construction\n",
    "with open(\"metadata_20251031.json\", \"r\") as f:\n",
    "    meta = pd.DataFrame(json.load(f)).set_index(\"screening_id\")\n",
    "with open(\"expert_features_zipformer_lm-extra06.json\", \"r\") as f:\n",
    "    features_raw = pd.DataFrame(json.load(f)).set_index(\"screening_id\")\n",
    "with open(\"train_20251031.json\", \"r\") as f:\n",
    "    train_ids = json.load(f)\n",
    "with open(\"test_20251031.json\", \"r\") as f:\n",
    "    test_ids = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ec35712-a4a4-40b5-b9cb-fab609b3979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "\n",
    "def get_xy(feature_names, target):\n",
    "    global features_raw\n",
    "    target_maps = {\"0vs23\": {0: 0, 2: 1, 3: 1}, \"0vs1vs2vs3\": {0: 0, 1: 1, 2: 2, 3: 3}}\n",
    "    targets = meta.loc[meta[\"kobar_kategorizace_definitivni\"].isin(target_maps[target]), \"kobar_kategorizace_definitivni\"].map(target_maps[target]).sort_index()\n",
    "    features = features_raw.loc[features_raw.index.intersection(targets.index)].sort_index()\n",
    "    x_train = features.loc[features.index.isin(train_ids)][feature_names]\n",
    "    x_test = features.loc[features.index.isin(test_ids)][feature_names]\n",
    "    y_train = targets.loc[targets.index.isin(train_ids)]\n",
    "    y_test = targets.loc[targets.index.isin(test_ids)]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def train_and_eval_logreg(x_train, y_train, x_test, y_test):\n",
    "    model = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"logreg\", LogisticRegression(random_state=42, max_iter=1000, solver=\"liblinear\")),\n",
    "    ])\n",
    "    model.fit(x_train, y_train)\n",
    "    predicted = model.predict(x_test)\n",
    "    report = classification_report(y_test, predicted, zero_division=0.0)\n",
    "    return report\n",
    "\n",
    "def train_and_eval_histgradboost(x_train, y_train, x_test, y_test):\n",
    "    model = HistGradientBoostingClassifier(random_state=42, early_stopping=False)\n",
    "    model.fit(x_train, y_train)\n",
    "    predicted = model.predict(x_test)\n",
    "    report = classification_report(y_test, predicted, zero_division=0.0)\n",
    "    return report\n",
    "\n",
    "def sep(): # just a simple separator snippet\n",
    "    print('='*80+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4f65f3f-6a2d-4f60-aa21-c8379eb6d5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 01] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_1_task1 Correctly repeated numbers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_1_task1 Correctly repeated numbers\n",
       "screening_id                                                                 \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                9.0\n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                9.0\n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                9.0\n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                9.0\n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                9.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 01] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82        42\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.35      0.50      0.41        60\n",
      "weighted avg       0.49      0.70      0.58        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 01] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.98      0.82        42\n",
      "           1       0.50      0.06      0.10        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.60      0.52      0.46        60\n",
      "weighted avg       0.64      0.70      0.60        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 01                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = ['expertFeatures_1_task1 Correctly repeated numbers']\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 01] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 01] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 01] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0d2cf8c-986d-4164-8425-c48ecabf3466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 02] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_2_task2 Correctly repeated characters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_2_task2 Correctly repeated characters\n",
       "screening_id                                                                    \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                4.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                4.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                4.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                4.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                3.0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 02] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82        42\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.35      0.50      0.41        60\n",
      "weighted avg       0.49      0.70      0.58        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 02] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82        42\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.35      0.50      0.41        60\n",
      "weighted avg       0.49      0.70      0.58        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 02                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = ['expertFeatures_2_task2 Correctly repeated characters']\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 02] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 02] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 02] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cface6e4-883d-42cb-a3c7-ef2676875dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 03] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_3_task3 Character match ratio</th>\n",
       "      <th>expertFeatures_4_task3 Correctly repeated words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>0.4490</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>0.9434</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>0.4227</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>0.6792</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_3_task3 Character match ratio  \\\n",
       "screening_id                                                               \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                        0.4490   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                        0.9434   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                        0.4227   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                        0.6792   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                        1.0000   \n",
       "\n",
       "                            expertFeatures_4_task3 Correctly repeated words  \n",
       "screening_id                                                                 \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                              6.0  \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                              9.0  \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                             10.0  \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                              9.0  \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                             10.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 03] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83        42\n",
      "           1       0.62      0.44      0.52        18\n",
      "\n",
      "    accuracy                           0.75        60\n",
      "   macro avg       0.70      0.66      0.67        60\n",
      "weighted avg       0.74      0.75      0.74        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 03] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        42\n",
      "           1       0.75      0.67      0.71        18\n",
      "\n",
      "    accuracy                           0.83        60\n",
      "   macro avg       0.81      0.79      0.79        60\n",
      "weighted avg       0.83      0.83      0.83        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 03                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_3_task3 Character match ratio',\n",
    "\t'expertFeatures_4_task3 Correctly repeated words'\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 03] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 03] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 03] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0cfac83b-83ca-4474-ae3f-d01b757d03e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 04m] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_5_task4 Sentence count</th>\n",
       "      <th>expertFeatures_6_task4 First person verb proportion</th>\n",
       "      <th>expertFeatures_7_task4 Meaningful words ratio</th>\n",
       "      <th>expertFeatures_8_task4 Pronoun to noun ratio</th>\n",
       "      <th>expertFeatures_9_task4 Count of repeated meaningful words</th>\n",
       "      <th>expertFeatures_10_task4 Unique words to total words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.638554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.587302</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_5_task4 Sentence count  \\\n",
       "screening_id                                                        \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                    6.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                    3.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                   10.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                   15.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                   15.0   \n",
       "\n",
       "                            expertFeatures_6_task4 First person verb proportion  \\\n",
       "screening_id                                                                      \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                           0.000000     \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                           0.166667     \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                           0.000000     \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                           0.043478     \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                           0.000000     \n",
       "\n",
       "                            expertFeatures_7_task4 Meaningful words ratio  \\\n",
       "screening_id                                                                \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                       0.573770   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                       0.647059   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                       0.595960   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                       0.587302   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                       0.538462   \n",
       "\n",
       "                            expertFeatures_8_task4 Pronoun to noun ratio  \\\n",
       "screening_id                                                               \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                      0.173913   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                      0.071429   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                      0.028571   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                      0.242424   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                      0.217391   \n",
       "\n",
       "                            expertFeatures_9_task4 Count of repeated meaningful words  \\\n",
       "screening_id                                                                            \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                6.0           \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                3.0           \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                               13.0           \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                               17.0           \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                8.0           \n",
       "\n",
       "                            expertFeatures_10_task4 Unique words to total words  \n",
       "screening_id                                                                     \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                           0.769231    \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                           0.896552    \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                           0.638554    \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                           0.615385    \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                           0.836066    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 04m] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82        42\n",
      "           1       0.57      0.44      0.50        18\n",
      "\n",
      "    accuracy                           0.73        60\n",
      "   macro avg       0.68      0.65      0.66        60\n",
      "weighted avg       0.72      0.73      0.72        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 04m] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        42\n",
      "           1       0.50      0.50      0.50        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.64      0.64      0.64        60\n",
      "weighted avg       0.70      0.70      0.70        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 04m                          #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_5_task4 Sentence count',\n",
    "\t'expertFeatures_6_task4 First person verb proportion',\n",
    "\t'expertFeatures_7_task4 Meaningful words ratio',\n",
    "\t'expertFeatures_8_task4 Pronoun to noun ratio',\n",
    "\t'expertFeatures_9_task4 Count of repeated meaningful words',\n",
    "\t'expertFeatures_10_task4 Unique words to total words',\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 04m] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 04m] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 04m] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12f87b2e-0a8b-4b56-b5a8-5e99af0768e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 04s] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_11_task4 Named object count</th>\n",
       "      <th>expertFeatures_12_task4 Described object relation count</th>\n",
       "      <th>expertFeatures_13_task4 Distinct topic count</th>\n",
       "      <th>expertFeatures_14_task4 Description trajectory length</th>\n",
       "      <th>expertFeatures_15_task4 Objects in water count</th>\n",
       "      <th>expertFeatures_16_task4 Objects in sky count</th>\n",
       "      <th>expertFeatures_17_task4 Objects on land count</th>\n",
       "      <th>expertFeatures_18_task4 Explicit child danger mentioned</th>\n",
       "      <th>expertFeatures_19_task4 Explicit animal danger mentioned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.866285</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.594183</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.319715</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.277062</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.562818</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_11_task4 Named object count  \\\n",
       "screening_id                                                             \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                        21.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                        13.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                        33.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                        24.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                        21.0   \n",
       "\n",
       "                            expertFeatures_12_task4 Described object relation count  \\\n",
       "screening_id                                                                          \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                               13.0         \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                8.0         \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                5.0         \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                5.0         \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                4.0         \n",
       "\n",
       "                            expertFeatures_13_task4 Distinct topic count  \\\n",
       "screening_id                                                               \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                          14.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                          13.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                          24.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                          16.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                          14.0   \n",
       "\n",
       "                            expertFeatures_14_task4 Description trajectory length  \\\n",
       "screening_id                                                                        \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                           0.866285       \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                           0.594183       \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                           2.319715       \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                           1.277062       \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                           1.562818       \n",
       "\n",
       "                            expertFeatures_15_task4 Objects in water count  \\\n",
       "screening_id                                                                 \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                             7.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                             3.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                             8.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                             5.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                             8.0   \n",
       "\n",
       "                            expertFeatures_16_task4 Objects in sky count  \\\n",
       "screening_id                                                               \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                           3.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                           2.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                           5.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                           5.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                           3.0   \n",
       "\n",
       "                            expertFeatures_17_task4 Objects on land count  \\\n",
       "screening_id                                                                \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                            5.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                            3.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                           11.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                            9.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                            4.0   \n",
       "\n",
       "                            expertFeatures_18_task4 Explicit child danger mentioned  \\\n",
       "screening_id                                                                          \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                0.0         \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                1.0         \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                0.0         \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                1.0         \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                1.0         \n",
       "\n",
       "                            expertFeatures_19_task4 Explicit animal danger mentioned  \n",
       "screening_id                                                                          \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                1.0         \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                0.0         \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                1.0         \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                1.0         \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                1.0         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 04s] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82        42\n",
      "           1       0.59      0.56      0.57        18\n",
      "\n",
      "    accuracy                           0.75        60\n",
      "   macro avg       0.70      0.69      0.70        60\n",
      "weighted avg       0.75      0.75      0.75        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 04s] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79        42\n",
      "           1       0.50      0.44      0.47        18\n",
      "\n",
      "    accuracy                           0.70        60\n",
      "   macro avg       0.64      0.63      0.63        60\n",
      "weighted avg       0.69      0.70      0.69        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 04s                          #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_11_task4 Named object count',\n",
    "\t'expertFeatures_12_task4 Described object relation count',\n",
    "\t'expertFeatures_13_task4 Distinct topic count',\n",
    "\t'expertFeatures_14_task4 Description trajectory length',\n",
    "\t'expertFeatures_15_task4 Objects in water count',\n",
    "\t'expertFeatures_16_task4 Objects in sky count',\n",
    "\t'expertFeatures_17_task4 Objects on land count',\n",
    "\t'expertFeatures_18_task4 Explicit child danger mentioned',\n",
    "\t'expertFeatures_19_task4 Explicit animal danger mentioned',\n",
    " ]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 04s] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 04s] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 04s] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d9f27fb-2ddf-4335-b6ef-4dc7dff9ee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 05] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_20_task5 Total recalled words count</th>\n",
       "      <th>expertFeatures_21_task5 Distinct objects recalled count</th>\n",
       "      <th>expertFeatures_22_task5 Repeated recalled words count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>42.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>25.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_20_task5 Total recalled words count  \\\n",
       "screening_id                                                                     \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                               42.0    \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                               25.0    \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                               28.0    \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                               20.0    \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                               24.0    \n",
       "\n",
       "                            expertFeatures_21_task5 Distinct objects recalled count  \\\n",
       "screening_id                                                                          \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                               11.0         \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                               11.0         \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                9.0         \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                3.0         \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                               11.0         \n",
       "\n",
       "                            expertFeatures_22_task5 Repeated recalled words count  \n",
       "screening_id                                                                       \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                9.0      \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                4.0      \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                0.0      \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                2.0      \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                4.0      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 05] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90        42\n",
      "           1       0.85      0.61      0.71        18\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.85      0.78      0.80        60\n",
      "weighted avg       0.85      0.85      0.84        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 05] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91        42\n",
      "           1       0.92      0.61      0.73        18\n",
      "\n",
      "    accuracy                           0.87        60\n",
      "   macro avg       0.89      0.79      0.82        60\n",
      "weighted avg       0.87      0.87      0.86        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 05                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_20_task5 Total recalled words count',\n",
    "\t'expertFeatures_21_task5 Distinct objects recalled count',\n",
    "\t'expertFeatures_22_task5 Repeated recalled words count',\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 05] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 05] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 05] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48d9aacf-f402-43a9-be1a-10367467f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 06] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_23_task6 Correctly named pictures count</th>\n",
       "      <th>expertFeatures_24_task6 Total naming reaction time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>15.0</td>\n",
       "      <td>73.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>17.0</td>\n",
       "      <td>57.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>16.0</td>\n",
       "      <td>59.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>17.0</td>\n",
       "      <td>57.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>19.0</td>\n",
       "      <td>50.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_23_task6 Correctly named pictures count  \\\n",
       "screening_id                                                                         \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                               15.0        \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                               17.0        \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                               16.0        \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                               17.0        \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                               19.0        \n",
       "\n",
       "                            expertFeatures_24_task6 Total naming reaction time  \n",
       "screening_id                                                                    \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                              73.88   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                              57.04   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                              59.60   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                              57.68   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                              50.80   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 06] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84        42\n",
      "           1       0.67      0.44      0.53        18\n",
      "\n",
      "    accuracy                           0.77        60\n",
      "   macro avg       0.73      0.67      0.69        60\n",
      "weighted avg       0.75      0.77      0.75        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 06] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85        42\n",
      "           1       0.69      0.50      0.58        18\n",
      "\n",
      "    accuracy                           0.78        60\n",
      "   macro avg       0.75      0.70      0.72        60\n",
      "weighted avg       0.77      0.78      0.77        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 06                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_23_task6 Correctly named pictures count',\n",
    "\t'expertFeatures_24_task6 Total naming reaction time',\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 06] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 06] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 06] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d757c35a-f78c-4e92-9b43-89ae2600448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 07] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_25_task7 Correctly recalled pictures count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_25_task7 Correctly recalled pictures count\n",
       "screening_id                                                                         \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                8.0        \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                4.0        \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                8.0        \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                1.0        \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                5.0        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 07] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89        42\n",
      "           1       0.76      0.72      0.74        18\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.82      0.81      0.82        60\n",
      "weighted avg       0.85      0.85      0.85        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 07] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86        42\n",
      "           1       0.65      0.83      0.73        18\n",
      "\n",
      "    accuracy                           0.82        60\n",
      "   macro avg       0.79      0.82      0.80        60\n",
      "weighted avg       0.84      0.82      0.82        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 07                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = ['expertFeatures_25_task7 Correctly recalled pictures count']\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 07] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 07] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 07] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8561d4e0-bb7d-498b-88a9-d33090d4bdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 08] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_26_task8 Total word count</th>\n",
       "      <th>expertFeatures_27_task8 Animal word count</th>\n",
       "      <th>expertFeatures_28_task8 Repeated animals count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_26_task8 Total word count  \\\n",
       "screening_id                                                           \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                      20.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                      16.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                      19.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                       8.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                      11.0   \n",
       "\n",
       "                            expertFeatures_27_task8 Animal word count  \\\n",
       "screening_id                                                            \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                       18.0   \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                       10.0   \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                       12.0   \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                        6.0   \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                       12.0   \n",
       "\n",
       "                            expertFeatures_28_task8 Repeated animals count  \n",
       "screening_id                                                                \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                             0.0  \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                             0.0  \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                             0.0  \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                             0.0  \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                             0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 08] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92        42\n",
      "           1       0.79      0.83      0.81        18\n",
      "\n",
      "    accuracy                           0.88        60\n",
      "   macro avg       0.86      0.87      0.86        60\n",
      "weighted avg       0.89      0.88      0.88        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 08] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92        42\n",
      "           1       0.82      0.78      0.80        18\n",
      "\n",
      "    accuracy                           0.88        60\n",
      "   macro avg       0.87      0.85      0.86        60\n",
      "weighted avg       0.88      0.88      0.88        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 08                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_26_task8 Total word count',\n",
    "\t'expertFeatures_27_task8 Animal word count',\n",
    "\t'expertFeatures_28_task8 Repeated animals count',\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 08] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 08] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 08] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80d9893e-a985-415e-9a21-a1e72d188c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 09] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_29_task9 Percentage of repeated-recalled sentence characters</th>\n",
       "      <th>expertFeatures_30_task9 Correct recalled sentence words count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>0.2830</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>0.4528</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>0.2073</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>0.1698</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>0.3623</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_29_task9 Percentage of repeated-recalled sentence characters  \\\n",
       "screening_id                                                                                              \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                             0.2830                             \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                             0.4528                             \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                             0.2073                             \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                             0.1698                             \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                             0.3623                             \n",
       "\n",
       "                            expertFeatures_30_task9 Correct recalled sentence words count  \n",
       "screening_id                                                                               \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                                2.0              \n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                                4.0              \n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                                0.0              \n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                                0.0              \n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                                5.0              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 09] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89        42\n",
      "           1       0.76      0.72      0.74        18\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.82      0.81      0.82        60\n",
      "weighted avg       0.85      0.85      0.85        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 09] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92        42\n",
      "           1       0.87      0.72      0.79        18\n",
      "\n",
      "    accuracy                           0.88        60\n",
      "   macro avg       0.88      0.84      0.85        60\n",
      "weighted avg       0.88      0.88      0.88        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 09                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = [\n",
    "\t'expertFeatures_29_task9 Percentage of repeated-recalled sentence characters',\n",
    "\t'expertFeatures_30_task9 Correct recalled sentence words count',\n",
    "]\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 09] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 09] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 09] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1006aacf-acc1-4d85-b608-409cc2a1bcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 10] training data (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expertFeatures_31_task10 Word similarity score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screening_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scr-29XBdG3UN32Lm22zGeq9AV</th>\n",
       "      <td>0.8040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2CMKXBDSreZoAkLDhcK6a8</th>\n",
       "      <td>0.8768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2UvzBxvgymGYCpdn2VnLUN</th>\n",
       "      <td>0.7285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2XfEpW35qx7wMfVFxJ9Tdp</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scr-2Xn65aWeAY3yHaAvy7Hd9y</th>\n",
       "      <td>0.5230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            expertFeatures_31_task10 Word similarity score\n",
       "screening_id                                                              \n",
       "scr-29XBdG3UN32Lm22zGeq9AV                                          0.8040\n",
       "scr-2CMKXBDSreZoAkLDhcK6a8                                          0.8768\n",
       "scr-2UvzBxvgymGYCpdn2VnLUN                                          0.7285\n",
       "scr-2XfEpW35qx7wMfVFxJ9Tdp                                             NaN\n",
       "scr-2Xn65aWeAY3yHaAvy7Hd9y                                          0.5230"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "[Task 10] classification report (Logistic Regression)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.93      0.80        42\n",
      "           1       0.25      0.06      0.09        18\n",
      "\n",
      "    accuracy                           0.67        60\n",
      "   macro avg       0.47      0.49      0.44        60\n",
      "weighted avg       0.56      0.67      0.58        60\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Task 10] classification report (Histogram Gradient Boosting)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.98      0.81        42\n",
      "           1       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.68        60\n",
      "   macro avg       0.35      0.49      0.41        60\n",
      "weighted avg       0.49      0.68      0.57        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                           TASK 10                           #\n",
    "###############################################################\n",
    "\n",
    "# define what feature names we want to include in the experiment\n",
    "task_features = ['expertFeatures_31_task10 Word similarity score']\n",
    "\n",
    "# obtain the filtered X-y splits for test and train subsets\n",
    "x_train, y_train, x_test, y_test = get_xy(task_features, \"0vs23\")\n",
    "\n",
    "# print the obtained training data (just a sample) to see what they actually look like\n",
    "print(\"[Task 10] training data (sample):\")\n",
    "display(x_train.head())\n",
    "sep()\n",
    "\n",
    "# train the LOGISTIC REGRESSION classifier and evaluate on the test subset\n",
    "report = train_and_eval_logreg(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# print the classificaion report\n",
    "print(\"[Task 10] classification report (Logistic Regression)\")\n",
    "print(report)\n",
    "sep()\n",
    "\n",
    "# now train the HISTOGRAM GRADIENT BOOSTING classifier and evaluate on the test subset\n",
    "report = train_and_eval_histgradboost(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# and again print the classification report\n",
    "print(\"[Task 10] classification report (Histogram Gradient Boosting)\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
